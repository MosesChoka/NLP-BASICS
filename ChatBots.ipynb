{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8aa49ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.4.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d81f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b445ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import spacy'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ===> INTJ\n",
      "Mum ===> PROPN\n",
      ", ===> PUNCT\n",
      "how ===> SCONJ\n",
      "are ===> AUX\n",
      "you ===> PRON\n",
      "feeling ===> VERB\n",
      "today ===> NOUN\n",
      "? ===> PUNCT\n"
     ]
    }
   ],
   "source": [
    "# POS-tagging\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc1 = nlp(u'Hello Mum, how are you feeling today?')\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text + \" ===>\", token.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be59929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ===> PRON\n",
      "am ===> AUX\n",
      "going ===> VERB\n",
      "on ===> ADP\n",
      "a ===> DET\n",
      "road ===> NOUN\n",
      "trip ===> NOUN\n",
      "with ===> ADP\n",
      "my ===> PRON\n",
      "friend ===> NOUN\n"
     ]
    }
   ],
   "source": [
    "# POS-tagging example 2\n",
    "doc2 = nlp(u'I am going on a road trip with my friend')\n",
    "for token in doc2:\n",
    "    print(token.text + \" ===>\",token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd4723f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You ===> PRON\n",
      "ai ===> AUX\n",
      "nt ===> PART\n",
      "shit ===> VERB\n",
      "without ===> ADP\n",
      "your ===> PRON\n",
      "home ===> NOUN\n",
      "boys ===> NOUN\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u'You aint shit without your home boys')\n",
    "for token in doc3:\n",
    "    print(token.text + \" ===>\", token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2176662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Google PROPN NNP compound Xxxxx True False\n",
      "release release NOUN NN ROOT xxxx True False\n",
      "\" \" PUNCT `` punct \" False False\n",
      "MOvie MOvie PROPN NNP compound XXxxx True False\n",
      "Mirror Mirror PROPN NNP appos Xxxxx True False\n",
      "\" \" PUNCT '' punct \" False False\n",
      "AI AI PROPN NNP compound XX True False\n",
      "experiment experiment NOUN NN dobj xxxx True False\n",
      "that that PRON WDT nsubj xxxx True True\n",
      "matches match VERB VBZ relcl xxxx True False\n",
      "your your PRON PRP$ poss xxxx True True\n",
      "pose pose NOUN NN dobj xxxx True False\n",
      "from from ADP IN prep xxxx True True\n",
      "80,000 80,000 NUM CD nummod dd,ddd False False\n",
      "images image NOUN NNS pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "# POS-tagging example 3\n",
    "doc3 = nlp(u'Google release \"MOvie Mirror\" AI experiment that matches your pose from 80,000 images')\n",
    "for token in doc3:\n",
    "    print(token.text, token.lemma_,token.pos_, token.tag_, token.dep_,token.shape_,token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed376421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ===> hello\n",
      ", ===> ,\n",
      "today ===> today\n",
      "is ===> be\n",
      "such ===> such\n",
      "a ===> a\n",
      "beautiful ===> beautiful\n",
      "day ===> day\n",
      "to ===> to\n",
      "learn ===> learn\n",
      "about ===> about\n",
      "chatbots ===> chatbot\n",
      "and ===> and\n",
      "there ===> there\n",
      "uses ===> use\n",
      ". ===> .\n"
     ]
    }
   ],
   "source": [
    "# Stemming and Lemmatization\n",
    "text = nlp(u'Hello, today is such a beautiful day to learn about chatbots and there uses.')\n",
    "\n",
    "for token in text:\n",
    "    print(token.text + \" ===>\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5f8b2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ===> hello\n",
      "Tuesday ===> Tuesday\n",
      ", ===> ,\n",
      "I ===> I\n",
      "am ===> be\n",
      "working ===> work\n",
      "so ===> so\n",
      "hard ===> hard\n"
     ]
    }
   ],
   "source": [
    "# stemming 2\n",
    "doc12 = nlp(u'Hello Tuesday, I am working so hard')\n",
    "\n",
    "for token in doc12:\n",
    "    print(token.text + \" ===>\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d53728a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ===> a\n",
      "letter ===> letter\n",
      "has ===> have\n",
      "been ===> be\n",
      "written ===> write\n",
      ", ===> ,\n",
      "asking ===> ask\n",
      "him ===> he\n",
      "to ===> to\n",
      "be ===> be\n",
      "released ===> release\n"
     ]
    }
   ],
   "source": [
    "# lemmatization example 2\n",
    "sentence2 = nlp(u'A letter has been written, asking him to be released')\n",
    "for word in sentence2:\n",
    "    print(word.text + ' ===>', word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bba9f98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer ===> computer\n",
      "compute ===> compute\n",
      "computing ===> computing\n",
      "computed ===> compute\n"
     ]
    }
   ],
   "source": [
    "sentence3 = nlp(u'computer compute computing computed')\n",
    "\n",
    "for token in sentence3:\n",
    "    print(token.text +\" ===>\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5b2fe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ===> ORG\n",
      "Mountain View ===> GPE\n",
      "California ===> GPE\n",
      "109.65 billion US dollars ===> MONEY\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition\n",
    "my_string = u'Google has its headquarters in Mountain View, California having revenue amounted to 109.65 billion US dollars'\n",
    "\n",
    "doc5 = nlp(my_string)\n",
    "\n",
    "for token in doc5.ents:\n",
    "    print(token.text + ' ===>', token.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af222b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kenya ===> GPE\n",
      "one ===> CARDINAL\n",
      "Moringa School ===> ORG\n"
     ]
    }
   ],
   "source": [
    "# NER 2\n",
    "doc5 =nlp(u'In Kenya there are few schools offering programming courses, one of the schools is Moringa School')\n",
    "\n",
    "for entity in doc5.ents:\n",
    "    print(entity.text +\" ===>\", entity.label_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9785196c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark Zuckerberg ===> PERSON\n",
      "May 14, 1984 ===> DATE\n",
      "New York ===> GPE\n",
      "American ===> NORP\n",
      "Facebook ===> PERSON\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition example 2\n",
    "my_string1 = u'Mark Zuckerberg born May 14, 1984 in New York is an American technology entrepreneur and philanthropist best known for co-founding and leading Facebook as its chairman and CEO'\n",
    "doc6 = nlp(my_string1)\n",
    "\n",
    "for ent in doc6.ents:\n",
    "    print(ent.text + ' ===>', ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d305224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:00 AM ===> TIME\n",
      "90% ===> PERCENT\n"
     ]
    }
   ],
   "source": [
    "# NER Example 3\n",
    "my_string2 = u'I usually wake up at 9:00 AM. 90% of my daytime goes in learning new things'\n",
    "doc7 = nlp(my_string2)\n",
    "\n",
    "for ent in doc7.ents:\n",
    "    print(ent.text + ' ===>', ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fa1bc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "December DATE\n",
      "50% PERCENT\n",
      "Olympics EVENT\n"
     ]
    }
   ],
   "source": [
    "# Named-Entity Recognition Example 4\n",
    "my_string3 = u'Many people born in December have a 50% chance of succeeding in the Olympics games'\n",
    "doc13 = nlp(my_string3)\n",
    "\n",
    "for doc in doc13.ents:\n",
    "    print(doc.text, doc.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "930b0afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millionaire Fastlane ===> WORK_OF_ART\n",
      "MJ Demarco ===> PERSON\n",
      "2011 ===> DATE\n"
     ]
    }
   ],
   "source": [
    "doc6 = nlp(u'I am currently reading \"Millionaire Fastlane\" by MJ Demarco, it is a good read for anyone who wants to be an entrepreneur. It was published in 2011')\n",
    "\n",
    "for token in doc6.ents:\n",
    "    print(token.text +\" ===>\", token.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52d38405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I am currently reading &quot;\n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Millionaire Fastlane\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       "&quot; by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    MJ Demarco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", it is a good read for anyone who wants to be an entrepreneur. It was published in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2011\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive visualization of entities\n",
    "from spacy import displacy\n",
    "displacy.render(doc6, style = 'ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "518e0cb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"'s\", 'anyway', 'am', 'if', 'something', 'nothing', 'could', 'can', 'part', 'throughout', 'any', 'none', 'few', 'used', 'yourself', 'else', 'meanwhile', 'full', 'side', 'every', 'beside', '‘s', 'until', 'was', \"'m\", 'where', 'while', 'out', \"'d\", 'amongst', 'does', 'of', 'them', 'each', 'hereby', 'our', 'unless', 'who', 'thus', 'over', 'otherwise', 'from', \"n't\", 'there', 'because', 'me', 'what', 'per', 'see', \"'ve\", 'hereafter', 'doing', 'other', 'they', 'the', 'seems', 'thence', 'anything', 'keep', 'about', 'indeed', 'whoever', 'through', 'after', 'serious', 'becoming', 'wherever', 'an', 'whose', 'please', 'somewhere', 'hence', 'except', 'third', 'than', 'everywhere', 'became', 'many', 'same', 'enough', 'call', 'n‘t', 'often', 'no', 'hereupon', 'toward', 'once', 'under', 'front', 'before', 'its', 'yet', 'hundred', 'take', 'both', 'do', 'get', 'will', 'amount', 'been', 'next', 'one', \"'re\", 'ourselves', 'say', 'thereafter', 'never', 'whereafter', 'during', 'we', 'it', 'may', 'did', '‘ve', 'her', 'much', 'i', 'afterwards', 'now', 'nine', 'always', 'whether', 'your', 'by', 'off', 'someone', 'to', 'bottom', 'nevertheless', 'sometimes', 'had', 'some', 'first', 'were', 'and', 'cannot', 'least', '‘ll', 'eight', 'on', 'whence', 'perhaps', 'made', 'fifteen', 'those', '‘d', '‘re', 'how', 'seem', 'or', 'done', 'own', 'whereby', 'less', 'formerly', 'must', 'yours', 'herself', 'anywhere', 'my', 'ca', 'give', 'empty', 'with', 'here', 're', 'ten', 'beforehand', 'his', 'himself', 'so', '’s', 'anyhow', 'elsewhere', 'further', 'ours', 'seeming', 'be', 'a', 'thereby', 'whom', 'make', 'you', 'three', 'most', 'well', 'whereupon', 'more', 'really', 'almost', 'nobody', 'everyone', 'without', 'alone', 'four', 'herein', 'below', 'former', 'forty', 'everything', 'as', 'twelve', 'again', 'has', 'however', '’ll', 'are', 'still', 'why', 'also', 'being', 'using', 'noone', 'therein', 'quite', 'which', 'moreover', 'itself', 'myself', '’m', 'against', 'within', 'among', 'would', 'become', 'various', 'go', 'only', 'yourselves', 'already', 'very', 'around', 'sometime', 'these', 'mostly', 'sixty', 'but', 'onto', 'hers', 'name', 'beyond', 'behind', 'five', 'this', 'thereupon', 'becomes', 'us', 'not', 'others', 'due', 'either', 'show', 'latterly', 'nowhere', 'ever', 'themselves', 'whither', 'anyone', 'several', 'along', 'thru', 'when', 'up', '’re', 'between', 'whatever', 'that', 'neither', 'into', 'besides', \"'ll\", 'all', 'eleven', 'nor', 'latter', 'namely', 'then', 'somehow', 'last', 'via', 'above', 'she', 'fifty', 'him', 'two', 'across', 'since', 'should', 'too', 'whereas', 'their', 'put', 'at', 'although', 'twenty', 'together', 'move', '‘m', 'might', 'n’t', 'six', 'another', 'rather', 'towards', 'therefore', 'even', '’ve', 'regarding', 'have', 'is', 'top', 'whenever', 'seemed', 'he', 'back', 'for', 'down', 'upon', 'in', 'though', 'mine', '’d', 'wherein', 'such', 'whole', 'just'}\n"
     ]
    }
   ],
   "source": [
    "# Stop Words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "901e9a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tommorow ===> False\n",
      "will ===> True\n",
      "be ===> True\n",
      "too ===> True\n",
      "late ===> False\n",
      ", ===> False\n",
      "its ===> True\n",
      "now ===> True\n",
      "or ===> True\n",
      "never ===> True\n"
     ]
    }
   ],
   "source": [
    "# Checking for stopping words\n",
    "doc14 = nlp(u'Tommorow will be too late, its now or never')\n",
    "\n",
    "for token in doc14:\n",
    "    print(token.text + \" ===>\",token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5b123da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import spacy'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text :  we will show how to remove stopwords using spacy library\n",
      "Text after removing stopwords :  remove stopwords spacy library\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords using Spacy\n",
    "en= spacy.load('en_core_web_sm')\n",
    "stopwords = en.Defaults.stop_words\n",
    "\n",
    "text =u'we will show how to remove stopwords using spacy library'\n",
    "\n",
    "lst = []\n",
    "\n",
    "for token in text.split():\n",
    "    if token.lower() not in stopwords:\n",
    "        lst.append(token)\n",
    "        \n",
    "# Join items in the list\n",
    "print(\"Original text : \", text)\n",
    "print(\"Text after removing stopwords : \", ' '.join(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30b0cf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import spacy'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text :  This feature of spaCy gives you a parsed tree that explains the parent-child relationship between the words or phrases and is independent of the order in which words occur.\n",
      "Text after removing stopwords :  feature spaCy gives parsed tree explains parent-child relationship words phrases independent order words occur.\n"
     ]
    }
   ],
   "source": [
    "en= spacy.load('en_core_web_sm')\n",
    "stopwords = en.Defaults.stop_words\n",
    "\n",
    "text =u'This feature of spaCy gives you a parsed tree that explains the parent-child relationship between the words or phrases and is independent of the order in which words occur.'\n",
    "\n",
    "lst = []\n",
    "\n",
    "for token in text.split():\n",
    "    if token.lower() not in stopwords:\n",
    "        lst.append(token)\n",
    "        \n",
    "# Join items in the list\n",
    "print(\"Original text : \", text)\n",
    "print(\"Text after removing stopwords : \", ' '.join(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99d777af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[from, flight, Book]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dependency Parsing\n",
    "txt = nlp(u'Book me a flight from Bangalore to Goa')\n",
    "blr, goa = txt[5], txt[7]\n",
    "list(blr.ancestors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbcc15a",
   "metadata": {},
   "source": [
    "The above output can tell us that the user is looking to book the flight from Bangalore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a5d0759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[to, flight, Book]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(goa.ancestors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf7ce12",
   "metadata": {},
   "source": [
    "The above output can tell us that the user is looking to book flight to Goa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "171738e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Book]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(txt[3].ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1e744ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[4].is_ancestor(txt[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03e9d706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[3].is_ancestor(txt[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "626ebb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Bangalore]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(txt[4].children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0d5a62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"65cf28fa494349bdb92031e2496bc90d-0\" class=\"displacy\" width=\"1850\" height=\"362.0\" direction=\"ltr\" style=\"max-width: none; height: 362.0px; color: white; background: #09a3d5; font-family: Source Sans Pro; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Book</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">table</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"800\">restaurant</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">taxi</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1400\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1550\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1550\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1700\">hotel</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1700\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-65cf28fa494349bdb92031e2496bc90d-0-0\" stroke-width=\"2px\" d=\"M212,227.0 212,202.0 344.0,202.0 344.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-65cf28fa494349bdb92031e2496bc90d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M212,229.0 L208,221.0 216,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-65cf28fa494349bdb92031e2496bc90d-0-1\" stroke-width=\"2px\" d=\"M62,227.0 62,177.0 347.0,177.0 347.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-65cf28fa494349bdb92031e2496bc90d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M347.0,229.0 L351.0,221.0 343.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-65cf28fa494349bdb92031e2496bc90d-0-2\" stroke-width=\"2px\" d=\"M362,227.0 362,202.0 494.0,202.0 494.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-65cf28fa494349bdb92031e2496bc90d-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M494.0,229.0 L498.0,221.0 490.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-65cf28fa494349bdb92031e2496bc90d-0-3\" stroke-width=\"2px\" d=\"M662,227.0 662,202.0 794.0,202.0 794.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-65cf28fa494349bdb92031e2496bc90d-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M662,229.0 L658,221.0 666,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-65cf28fa494349bdb92031e2496bc90d-0-4\" stroke-width=\"2px\" d=\"M512,227.0 512,177.0 797.0,177.0 797.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-65cf28fa494349bdb92031e2496bc90d-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M797.0,229.0 L801.0,221.0 793.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-65cf28fa494349bdb92031e2496bc90d-0-5\" stroke-width=\"2px\" d=\"M812,227.0 812,202.0 944.0,202.0 944.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-65cf28fa494349bdb92031e2496bc90d-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M944.0,229.0 L948.0,221.0 940.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-65cf28fa494349bdb92031e2496bc90d-0-6\" stroke-width=\"2px\" d=\"M1112,227.0 1112,202.0 1244.0,202.0 1244.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-65cf28fa494349bdb92031e2496bc90d-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1112,229.0 L1108,221.0 1116,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-65cf28fa494349bdb92031e2496bc90d-0-7\" stroke-width=\"2px\" d=\"M812,227.0 812,177.0 1247.0,177.0 1247.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-65cf28fa494349bdb92031e2496bc90d-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1247.0,229.0 L1251.0,221.0 1243.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-65cf28fa494349bdb92031e2496bc90d-0-8\" stroke-width=\"2px\" d=\"M62,227.0 62,152.0 1400.0,152.0 1400.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-65cf28fa494349bdb92031e2496bc90d-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1400.0,229.0 L1404.0,221.0 1396.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-65cf28fa494349bdb92031e2496bc90d-0-9\" stroke-width=\"2px\" d=\"M1562,227.0 1562,202.0 1694.0,202.0 1694.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-65cf28fa494349bdb92031e2496bc90d-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1562,229.0 L1558,221.0 1566,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-65cf28fa494349bdb92031e2496bc90d-0-10\" stroke-width=\"2px\" d=\"M1412,227.0 1412,177.0 1697.0,177.0 1697.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-65cf28fa494349bdb92031e2496bc90d-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1697.0,229.0 L1701.0,221.0 1693.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Interactive Visualization of dependency parsing\n",
    "from spacy import displacy\n",
    "\n",
    "doc7 = nlp('Book a table at the restaurant and the taxi to the hotel')\n",
    "\n",
    "options = {\"compact\": True, \"bg\": \"#09a3d5\",\n",
    "           \"color\": \"white\", \"font\": \"Source Sans Pro\"}\n",
    "\n",
    "displacy.render(doc7, style='dep', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "407138ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Boston Dynamics, thousands, robot dogs]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Noun chunking / NP chunking\n",
    "string = nlp(u'Boston Dynamics is gearing up to produce thousands of robot dogs')\n",
    "\n",
    "list(string.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56acdaaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning ===> learning ===> nsubj ===>  cracks\n",
      "the code ===> code ===> dobj ===>  cracks\n",
      "messenger RNAs ===> RNAs ===> pobj ===>  of\n",
      "protein coding potential ===> potential ===> dobj ===>  cracks\n"
     ]
    }
   ],
   "source": [
    "string2 = nlp(u'Deep learning cracks the code of messenger RNAs and protein coding potential')\n",
    "\n",
    "for chunk in string2.noun_chunks:\n",
    "    print(chunk.text + \" ===>\", chunk.root.text + \" ===>\", chunk.root.dep_ + \" ===> \",chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb6846e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How [ 1.4228827  -0.47080153 -0.0077582   0.15813464 -0.49182713]\n",
      "are [ 2.7373657  -0.7749263  -0.40102917 -1.2484192   1.128918  ]\n",
      "you [-0.21479104 -0.6785506  -0.77575135 -0.8879999   0.28091776]\n",
      "doing [-1.1134546   1.82159    -0.19915128  1.4674124   0.01566988]\n",
      "today [-0.40161788 -0.9311397   0.17077836 -1.9610596  -1.6717634 ]\n",
      "? [-1.6762066  -0.17242669  0.17969191  0.24217647 -0.9138102 ]\n"
     ]
    }
   ],
   "source": [
    "# Finding Similarity\n",
    "doc10 = nlp(u'How are you doing today?')\n",
    "\n",
    "for token in doc10:\n",
    "    print(token.text, token.vector[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2151cae0",
   "metadata": {},
   "source": [
    "To find similarity between two words in spaCy we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13b28b13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6473686254886916\n",
      "0.42784187775653376\n",
      "0.1569142779241427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5700\\1316168498.py:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(hello_doc.similarity(hi_doc))\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5700\\1316168498.py:5: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(hello_doc.similarity(hella_doc))\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5700\\1316168498.py:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(hi_doc.similarity(hella_doc))\n"
     ]
    }
   ],
   "source": [
    "hello_doc = nlp(u\"hello\")\n",
    "hi_doc = nlp(u\"hi\")\n",
    "hella_doc = nlp(u\"hella\")\n",
    "print(hello_doc.similarity(hi_doc))\n",
    "print(hello_doc.similarity(hella_doc))\n",
    "print(hi_doc.similarity(hella_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61204139",
   "metadata": {},
   "source": [
    "If you see the word hello, it's more related and similar to the word hi (0.647/65%), even though there's only a difference of a character between the words hello and hella, the similarity is small (0.428/43%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28b40bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5700\\3912059188.py:5: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  GoT_1.similarity(GoT_2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5757127633078118"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding similarity in sentences\n",
    "GoT_1 = nlp(u\"When will next season of Game of Thrones be releasing?\")\n",
    "GoT_2 = nlp(u\"Game of Thrones next season release date?\")\n",
    "\n",
    "GoT_1.similarity(GoT_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7ce8e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5700\\2609963909.py:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  C_Date1.similarity(C_Date2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5510638904239246"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_Date1= nlp(u'When are you closing school?')\n",
    "C_Date2 = nlp(u'You are closing school on which date?')\n",
    "\n",
    "C_Date1.similarity(C_Date2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7086bf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word car is 100% similar to word car\n",
      "Word car is 47% similar to word truck\n",
      "Word car is -5% similar to word google\n",
      "Word truck is 47% similar to word car\n",
      "Word truck is 100% similar to word truck\n",
      "Word truck is 10% similar to word google\n",
      "Word google is -5% similar to word car\n",
      "Word google is 10% similar to word truck\n",
      "Word google is 100% similar to word google\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5700\\3929958156.py:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity_perc = int(t1.similarity(t2) * 100)\n"
     ]
    }
   ],
   "source": [
    "# finding similarity between words\n",
    "example_doc = nlp(u\"car truck google\")\n",
    "\n",
    "for t1 in example_doc:\n",
    "    for t2 in example_doc:\n",
    "        similarity_perc = int(t1.similarity(t2) * 100)\n",
    "        print(\"Word {} is {}% similar to word {}\".format(t1.text, similarity_perc, t2.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd972b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from_to pattern matched correctly. Printing values\n",
      "\n",
      "From: Airport Station, To: Hong Kong Station.\n"
     ]
    }
   ],
   "source": [
    "# Regular Expressions\n",
    "sentence1 = \"Book me a metro from Airport Station to Hong Kong Station.\"\n",
    "sentence2 = \"Book me a cab to Hong Kong Airport from AsiaWorld-Expo.\"\n",
    "\n",
    "import re\n",
    "from_to = re.compile('.* from (.*) to (.*)')\n",
    "to_from = re.compile('.* to (.*) from (.*)')\n",
    "\n",
    "\n",
    "from_to_match = from_to.match(sentence1)\n",
    "to_from_match = to_from.match(sentence1)\n",
    "\n",
    "if from_to_match and from_to_match.groups():\n",
    "    _from = from_to_match.groups()[0]\n",
    "    _to = from_to_match.groups()[1]\n",
    "    print(\"from_to pattern matched correctly. Printing values\\n\")\n",
    "    print(\"From: {}, To: {}\".format(_from,_to))\n",
    "    \n",
    "elif to_from_match and to_from_match.groups():\n",
    "    _to = to_from_match.groups()[0]\n",
    "    _from = to_from_match.group()[1]\n",
    "    print(\"to_from pattern matched correctly. Printing values\\n\")\n",
    "    print(\"From: {}, To: {}\".format(_from, _to))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
